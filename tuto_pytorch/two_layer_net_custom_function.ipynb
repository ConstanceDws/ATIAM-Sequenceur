{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining New autograd Functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 53876652.0)\n",
      "(1, 65930952.0)\n",
      "(2, 77420240.0)\n",
      "(3, 62575180.0)\n",
      "(4, 28388856.0)\n",
      "(5, 8270014.0)\n",
      "(6, 2931719.0)\n",
      "(7, 1763309.75)\n",
      "(8, 1347105.125)\n",
      "(9, 1096147.25)\n",
      "(10, 910341.4375)\n",
      "(11, 764422.875)\n",
      "(12, 647642.0625)\n",
      "(13, 552813.6875)\n",
      "(14, 474895.0)\n",
      "(15, 410432.9375)\n",
      "(16, 356589.875)\n",
      "(17, 311327.5625)\n",
      "(18, 272996.03125)\n",
      "(19, 240334.15625)\n",
      "(20, 212366.453125)\n",
      "(21, 188335.484375)\n",
      "(22, 167569.234375)\n",
      "(23, 149533.15625)\n",
      "(24, 133791.84375)\n",
      "(25, 120006.03125)\n",
      "(26, 107888.5625)\n",
      "(27, 97202.9140625)\n",
      "(28, 87757.1796875)\n",
      "(29, 79395.9609375)\n",
      "(30, 71973.796875)\n",
      "(31, 65343.60546875)\n",
      "(32, 59423.671875)\n",
      "(33, 54128.4921875)\n",
      "(34, 49379.64453125)\n",
      "(35, 45110.8203125)\n",
      "(36, 41265.625)\n",
      "(37, 37796.28125)\n",
      "(38, 34662.1328125)\n",
      "(39, 31825.974609375)\n",
      "(40, 29255.513671875)\n",
      "(41, 26920.697265625)\n",
      "(42, 24798.83984375)\n",
      "(43, 22867.033203125)\n",
      "(44, 21104.916015625)\n",
      "(45, 19495.912109375)\n",
      "(46, 18024.599609375)\n",
      "(47, 16678.791015625)\n",
      "(48, 15444.99609375)\n",
      "(49, 14314.8984375)\n",
      "(50, 13277.9150390625)\n",
      "(51, 12324.76171875)\n",
      "(52, 11447.6162109375)\n",
      "(53, 10639.7470703125)\n",
      "(54, 9894.958984375)\n",
      "(55, 9207.7734375)\n",
      "(56, 8573.09375)\n",
      "(57, 7986.7626953125)\n",
      "(58, 7444.57568359375)\n",
      "(59, 6942.45751953125)\n",
      "(60, 6477.3193359375)\n",
      "(61, 6046.4775390625)\n",
      "(62, 5646.810546875)\n",
      "(63, 5275.9150390625)\n",
      "(64, 4931.60791015625)\n",
      "(65, 4611.6796875)\n",
      "(66, 4314.1328125)\n",
      "(67, 4037.397216796875)\n",
      "(68, 3779.843505859375)\n",
      "(69, 3540.069091796875)\n",
      "(70, 3316.6689453125)\n",
      "(71, 3108.6015625)\n",
      "(72, 2914.392333984375)\n",
      "(73, 2733.049072265625)\n",
      "(74, 2563.79833984375)\n",
      "(75, 2405.74853515625)\n",
      "(76, 2258.12744140625)\n",
      "(77, 2120.078369140625)\n",
      "(78, 1991.0386962890625)\n",
      "(79, 1870.338623046875)\n",
      "(80, 1757.3961181640625)\n",
      "(81, 1651.684814453125)\n",
      "(82, 1552.68115234375)\n",
      "(83, 1459.9423828125)\n",
      "(84, 1373.087646484375)\n",
      "(85, 1291.6529541015625)\n",
      "(86, 1215.3826904296875)\n",
      "(87, 1143.8050537109375)\n",
      "(88, 1076.642822265625)\n",
      "(89, 1013.6519775390625)\n",
      "(90, 954.4920654296875)\n",
      "(91, 898.9599609375)\n",
      "(92, 846.820556640625)\n",
      "(93, 797.8458251953125)\n",
      "(94, 751.8343505859375)\n",
      "(95, 708.560791015625)\n",
      "(96, 667.8853149414062)\n",
      "(97, 629.6597900390625)\n",
      "(98, 593.7034301757812)\n",
      "(99, 559.87158203125)\n",
      "(100, 528.03076171875)\n",
      "(101, 498.09442138671875)\n",
      "(102, 469.9211120605469)\n",
      "(103, 443.3761901855469)\n",
      "(104, 418.38787841796875)\n",
      "(105, 394.85174560546875)\n",
      "(106, 372.68890380859375)\n",
      "(107, 351.8114013671875)\n",
      "(108, 332.1434631347656)\n",
      "(109, 313.6463623046875)\n",
      "(110, 296.2222900390625)\n",
      "(111, 279.8031005859375)\n",
      "(112, 264.3193054199219)\n",
      "(113, 249.72198486328125)\n",
      "(114, 235.9475860595703)\n",
      "(115, 222.96188354492188)\n",
      "(116, 210.70509338378906)\n",
      "(117, 199.15089416503906)\n",
      "(118, 188.24423217773438)\n",
      "(119, 177.95069885253906)\n",
      "(120, 168.23440551757812)\n",
      "(121, 159.06124877929688)\n",
      "(122, 150.3988494873047)\n",
      "(123, 142.22401428222656)\n",
      "(124, 134.5043487548828)\n",
      "(125, 127.21602630615234)\n",
      "(126, 120.33574676513672)\n",
      "(127, 113.82784271240234)\n",
      "(128, 107.6830062866211)\n",
      "(129, 101.8778305053711)\n",
      "(130, 96.3922348022461)\n",
      "(131, 91.2076187133789)\n",
      "(132, 86.30697631835938)\n",
      "(133, 81.67459106445312)\n",
      "(134, 77.2999496459961)\n",
      "(135, 73.16421508789062)\n",
      "(136, 69.25013732910156)\n",
      "(137, 65.5516357421875)\n",
      "(138, 62.05326843261719)\n",
      "(139, 58.74466323852539)\n",
      "(140, 55.6183967590332)\n",
      "(141, 52.659034729003906)\n",
      "(142, 49.86035919189453)\n",
      "(143, 47.21476364135742)\n",
      "(144, 44.71113586425781)\n",
      "(145, 42.34354782104492)\n",
      "(146, 40.10160827636719)\n",
      "(147, 37.97988510131836)\n",
      "(148, 35.97258377075195)\n",
      "(149, 34.07427215576172)\n",
      "(150, 32.27727508544922)\n",
      "(151, 30.57688331604004)\n",
      "(152, 28.967025756835938)\n",
      "(153, 27.443288803100586)\n",
      "(154, 25.999755859375)\n",
      "(155, 24.634838104248047)\n",
      "(156, 23.342689514160156)\n",
      "(157, 22.117687225341797)\n",
      "(158, 20.95814323425293)\n",
      "(159, 19.860870361328125)\n",
      "(160, 18.82145881652832)\n",
      "(161, 17.838342666625977)\n",
      "(162, 16.906858444213867)\n",
      "(163, 16.024330139160156)\n",
      "(164, 15.188284873962402)\n",
      "(165, 14.39684009552002)\n",
      "(166, 13.646517753601074)\n",
      "(167, 12.936807632446289)\n",
      "(168, 12.263535499572754)\n",
      "(169, 11.625988960266113)\n",
      "(170, 11.021785736083984)\n",
      "(171, 10.449745178222656)\n",
      "(172, 9.907547950744629)\n",
      "(173, 9.39391040802002)\n",
      "(174, 8.907323837280273)\n",
      "(175, 8.445892333984375)\n",
      "(176, 8.00879192352295)\n",
      "(177, 7.594529628753662)\n",
      "(178, 7.201937198638916)\n",
      "(179, 6.83032751083374)\n",
      "(180, 6.477591514587402)\n",
      "(181, 6.14341926574707)\n",
      "(182, 5.826663970947266)\n",
      "(183, 5.526419639587402)\n",
      "(184, 5.241690158843994)\n",
      "(185, 4.9719343185424805)\n",
      "(186, 4.71600341796875)\n",
      "(187, 4.4737114906311035)\n",
      "(188, 4.243880748748779)\n",
      "(189, 4.025703430175781)\n",
      "(190, 3.8189473152160645)\n",
      "(191, 3.623072624206543)\n",
      "(192, 3.4372758865356445)\n",
      "(193, 3.261101245880127)\n",
      "(194, 3.094031572341919)\n",
      "(195, 2.9355831146240234)\n",
      "(196, 2.785245895385742)\n",
      "(197, 2.6429216861724854)\n",
      "(198, 2.507739543914795)\n",
      "(199, 2.379490613937378)\n",
      "(200, 2.2579498291015625)\n",
      "(201, 2.1426315307617188)\n",
      "(202, 2.0334794521331787)\n",
      "(203, 1.9296382665634155)\n",
      "(204, 1.8313603401184082)\n",
      "(205, 1.7380201816558838)\n",
      "(206, 1.6492559909820557)\n",
      "(207, 1.5653101205825806)\n",
      "(208, 1.4856197834014893)\n",
      "(209, 1.410112738609314)\n",
      "(210, 1.338444709777832)\n",
      "(211, 1.2704548835754395)\n",
      "(212, 1.2057958841323853)\n",
      "(213, 1.144594669342041)\n",
      "(214, 1.0866526365280151)\n",
      "(215, 1.0313185453414917)\n",
      "(216, 0.9790891408920288)\n",
      "(217, 0.9293473362922668)\n",
      "(218, 0.8823286890983582)\n",
      "(219, 0.8375969529151917)\n",
      "(220, 0.7951836585998535)\n",
      "(221, 0.7549213767051697)\n",
      "(222, 0.7166480422019958)\n",
      "(223, 0.6804383993148804)\n",
      "(224, 0.6460718512535095)\n",
      "(225, 0.6134808659553528)\n",
      "(226, 0.5824568271636963)\n",
      "(227, 0.5529778003692627)\n",
      "(228, 0.5250405669212341)\n",
      "(229, 0.4985632300376892)\n",
      "(230, 0.4734364449977875)\n",
      "(231, 0.4495442807674408)\n",
      "(232, 0.42686983942985535)\n",
      "(233, 0.40537527203559875)\n",
      "(234, 0.3849470317363739)\n",
      "(235, 0.3655659556388855)\n",
      "(236, 0.3470976650714874)\n",
      "(237, 0.32963311672210693)\n",
      "(238, 0.31307682394981384)\n",
      "(239, 0.2972698509693146)\n",
      "(240, 0.2823624610900879)\n",
      "(241, 0.26818662881851196)\n",
      "(242, 0.25472527742385864)\n",
      "(243, 0.24192866683006287)\n",
      "(244, 0.2297305464744568)\n",
      "(245, 0.2181994616985321)\n",
      "(246, 0.2072715312242508)\n",
      "(247, 0.19685479998588562)\n",
      "(248, 0.1869872510433197)\n",
      "(249, 0.17760609090328217)\n",
      "(250, 0.1687101423740387)\n",
      "(251, 0.1602204144001007)\n",
      "(252, 0.15221668779850006)\n",
      "(253, 0.1446082592010498)\n",
      "(254, 0.13736127316951752)\n",
      "(255, 0.1304890364408493)\n",
      "(256, 0.1239674761891365)\n",
      "(257, 0.11776989698410034)\n",
      "(258, 0.11186642944812775)\n",
      "(259, 0.10624776780605316)\n",
      "(260, 0.10095873475074768)\n",
      "(261, 0.09591858088970184)\n",
      "(262, 0.09114518761634827)\n",
      "(263, 0.08658231049776077)\n",
      "(264, 0.08227735757827759)\n",
      "(265, 0.07814960926771164)\n",
      "(266, 0.07426374405622482)\n",
      "(267, 0.07056011259555817)\n",
      "(268, 0.06704872846603394)\n",
      "(269, 0.06372449547052383)\n",
      "(270, 0.060568444430828094)\n",
      "(271, 0.05752234905958176)\n",
      "(272, 0.05465182289481163)\n",
      "(273, 0.051942914724349976)\n",
      "(274, 0.04934931546449661)\n",
      "(275, 0.04689757153391838)\n",
      "(276, 0.04456092417240143)\n",
      "(277, 0.04234885796904564)\n",
      "(278, 0.04023760184645653)\n",
      "(279, 0.03824058920145035)\n",
      "(280, 0.03636585921049118)\n",
      "(281, 0.034566935151815414)\n",
      "(282, 0.032851070165634155)\n",
      "(283, 0.031222185119986534)\n",
      "(284, 0.029683014377951622)\n",
      "(285, 0.028224650770425797)\n",
      "(286, 0.026825323700904846)\n",
      "(287, 0.025494955480098724)\n",
      "(288, 0.024237068369984627)\n",
      "(289, 0.02303425408899784)\n",
      "(290, 0.02188928611576557)\n",
      "(291, 0.020820386707782745)\n",
      "(292, 0.01978275738656521)\n",
      "(293, 0.01881691999733448)\n",
      "(294, 0.017894849181175232)\n",
      "(295, 0.017010115087032318)\n",
      "(296, 0.01617741771042347)\n",
      "(297, 0.015389509499073029)\n",
      "(298, 0.014640675857663155)\n",
      "(299, 0.01392136700451374)\n",
      "(300, 0.013247772119939327)\n",
      "(301, 0.01259768009185791)\n",
      "(302, 0.011976205743849277)\n",
      "(303, 0.011399098671972752)\n",
      "(304, 0.010843365453183651)\n",
      "(305, 0.010318472981452942)\n",
      "(306, 0.009823373518884182)\n",
      "(307, 0.009360241703689098)\n",
      "(308, 0.00889930222183466)\n",
      "(309, 0.008479412645101547)\n",
      "(310, 0.00806999672204256)\n",
      "(311, 0.007684058975428343)\n",
      "(312, 0.007321684155613184)\n",
      "(313, 0.0069718495942652225)\n",
      "(314, 0.006636596750468016)\n",
      "(315, 0.006321263499557972)\n",
      "(316, 0.0060234577395021915)\n",
      "(317, 0.005740603897720575)\n",
      "(318, 0.005466845817863941)\n",
      "(319, 0.00521222036331892)\n",
      "(320, 0.00497057382017374)\n",
      "(321, 0.004741314798593521)\n",
      "(322, 0.004514574073255062)\n",
      "(323, 0.004308702424168587)\n",
      "(324, 0.004107730928808451)\n",
      "(325, 0.003918294794857502)\n",
      "(326, 0.003741546766832471)\n",
      "(327, 0.0035741766914725304)\n",
      "(328, 0.003407942596822977)\n",
      "(329, 0.0032532154582440853)\n",
      "(330, 0.0031087230890989304)\n",
      "(331, 0.002968776971101761)\n",
      "(332, 0.0028325014282017946)\n",
      "(333, 0.002707099076360464)\n",
      "(334, 0.0025880346074700356)\n",
      "(335, 0.002472277032211423)\n",
      "(336, 0.0023636005353182554)\n",
      "(337, 0.0022621643729507923)\n",
      "(338, 0.002166423946619034)\n",
      "(339, 0.0020717806182801723)\n",
      "(340, 0.001983399037271738)\n",
      "(341, 0.0018987314542755485)\n",
      "(342, 0.0018150057876482606)\n",
      "(343, 0.0017416581977158785)\n",
      "(344, 0.0016718702390789986)\n",
      "(345, 0.0016036865999922156)\n",
      "(346, 0.0015387972816824913)\n",
      "(347, 0.0014743328792974353)\n",
      "(348, 0.001414939877577126)\n",
      "(349, 0.0013563755201175809)\n",
      "(350, 0.0013040007324889302)\n",
      "(351, 0.001252562738955021)\n",
      "(352, 0.0012024474563077092)\n",
      "(353, 0.0011549618793651462)\n",
      "(354, 0.001110258512198925)\n",
      "(355, 0.0010665374575182796)\n",
      "(356, 0.001027529127895832)\n",
      "(357, 0.0009886129992082715)\n",
      "(358, 0.000951795547734946)\n",
      "(359, 0.0009178793989121914)\n",
      "(360, 0.0008829005528241396)\n",
      "(361, 0.0008511625346727669)\n",
      "(362, 0.0008202490862458944)\n",
      "(363, 0.0007905845995992422)\n",
      "(364, 0.0007621771655976772)\n",
      "(365, 0.0007356287678703666)\n",
      "(366, 0.0007093361346051097)\n",
      "(367, 0.0006849745404906571)\n",
      "(368, 0.0006617011968046427)\n",
      "(369, 0.000638921745121479)\n",
      "(370, 0.0006184299709275365)\n",
      "(371, 0.0005974668310955167)\n",
      "(372, 0.0005777246551588178)\n",
      "(373, 0.0005584970931522548)\n",
      "(374, 0.0005409012665040791)\n",
      "(375, 0.0005232789553701878)\n",
      "(376, 0.0005061552510596812)\n",
      "(377, 0.0004899397026747465)\n",
      "(378, 0.00047446839744225144)\n",
      "(379, 0.00045927127939648926)\n",
      "(380, 0.00044510996667668223)\n",
      "(381, 0.00043070639367215335)\n",
      "(382, 0.0004175395588390529)\n",
      "(383, 0.00040444082696922123)\n",
      "(384, 0.00039215522701852024)\n",
      "(385, 0.0003809685294982046)\n",
      "(386, 0.0003698508662637323)\n",
      "(387, 0.0003581814526114613)\n",
      "(388, 0.00034771664650179446)\n",
      "(389, 0.00033867682213895023)\n",
      "(390, 0.00032906129490584135)\n",
      "(391, 0.0003191918949596584)\n",
      "(392, 0.00030976743437349796)\n",
      "(393, 0.00030163658084347844)\n",
      "(394, 0.000294361321721226)\n",
      "(395, 0.00028599408688023686)\n",
      "(396, 0.0002785033721011132)\n",
      "(397, 0.00027101062005385756)\n",
      "(398, 0.0002645832719281316)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 0.00025687325978651643)\n",
      "(400, 0.00025022501358762383)\n",
      "(401, 0.00024361495161429048)\n",
      "(402, 0.00023721855541225523)\n",
      "(403, 0.00023142836289480329)\n",
      "(404, 0.00022487602836918086)\n",
      "(405, 0.00021950846712570637)\n",
      "(406, 0.00021400283731054515)\n",
      "(407, 0.0002087446191580966)\n",
      "(408, 0.00020411524747032672)\n",
      "(409, 0.00019940112542826682)\n",
      "(410, 0.00019470367988105863)\n",
      "(411, 0.00019012243137694895)\n",
      "(412, 0.000185363955097273)\n",
      "(413, 0.00018152265693061054)\n",
      "(414, 0.00017678714357316494)\n",
      "(415, 0.00017312927229795605)\n",
      "(416, 0.00016954097372945398)\n",
      "(417, 0.00016549447900615633)\n",
      "(418, 0.00016174744814634323)\n",
      "(419, 0.0001582654658704996)\n",
      "(420, 0.00015504087787121534)\n",
      "(421, 0.00015173589054029435)\n",
      "(422, 0.00014854328765068203)\n",
      "(423, 0.00014535074296873063)\n",
      "(424, 0.0001424104702891782)\n",
      "(425, 0.00013946708349976689)\n",
      "(426, 0.0001369277888443321)\n",
      "(427, 0.00013397334259934723)\n",
      "(428, 0.00013101134391035885)\n",
      "(429, 0.00012832377979066223)\n",
      "(430, 0.00012584215437527746)\n",
      "(431, 0.00012317977962084115)\n",
      "(432, 0.00012107559450669214)\n",
      "(433, 0.00011861677921842784)\n",
      "(434, 0.0001160117462859489)\n",
      "(435, 0.00011378632189007476)\n",
      "(436, 0.00011169887147843838)\n",
      "(437, 0.00010950853175017983)\n",
      "(438, 0.00010758799180621281)\n",
      "(439, 0.00010574836778687313)\n",
      "(440, 0.00010406668297946453)\n",
      "(441, 0.00010198443487752229)\n",
      "(442, 0.00010011250560637563)\n",
      "(443, 9.850897185970098e-05)\n",
      "(444, 9.689545549917966e-05)\n",
      "(445, 9.493336983723566e-05)\n",
      "(446, 9.323342237621546e-05)\n",
      "(447, 9.187444084091112e-05)\n",
      "(448, 9.008873894345015e-05)\n",
      "(449, 8.849754522088915e-05)\n",
      "(450, 8.707690722076222e-05)\n",
      "(451, 8.547188917873427e-05)\n",
      "(452, 8.433185575995594e-05)\n",
      "(453, 8.282151247840375e-05)\n",
      "(454, 8.13246369943954e-05)\n",
      "(455, 8.018000517040491e-05)\n",
      "(456, 7.87493700045161e-05)\n",
      "(457, 7.721285510342568e-05)\n",
      "(458, 7.612062472617254e-05)\n",
      "(459, 7.519921928178519e-05)\n",
      "(460, 7.379455928457901e-05)\n",
      "(461, 7.255672971950844e-05)\n",
      "(462, 7.14832276571542e-05)\n",
      "(463, 7.054174056975171e-05)\n",
      "(464, 6.947531073819846e-05)\n",
      "(465, 6.837226828793064e-05)\n",
      "(466, 6.724214472342283e-05)\n",
      "(467, 6.638682680204511e-05)\n",
      "(468, 6.525692879222333e-05)\n",
      "(469, 6.439774733735248e-05)\n",
      "(470, 6.356811354635283e-05)\n",
      "(471, 6.247527926461771e-05)\n",
      "(472, 6.145051884232089e-05)\n",
      "(473, 6.051940727047622e-05)\n",
      "(474, 5.9763322497019544e-05)\n",
      "(475, 5.870815220987424e-05)\n",
      "(476, 5.813660391140729e-05)\n",
      "(477, 5.756844984716736e-05)\n",
      "(478, 5.646956560667604e-05)\n",
      "(479, 5.608166247839108e-05)\n",
      "(480, 5.524592779693194e-05)\n",
      "(481, 5.4562511650146917e-05)\n",
      "(482, 5.3952200687490404e-05)\n",
      "(483, 5.3201845730654895e-05)\n",
      "(484, 5.24778151884675e-05)\n",
      "(485, 5.1954521040897816e-05)\n",
      "(486, 5.100935959490016e-05)\n",
      "(487, 5.045482612331398e-05)\n",
      "(488, 4.973590330337174e-05)\n",
      "(489, 4.926953261019662e-05)\n",
      "(490, 4.862752030021511e-05)\n",
      "(491, 4.8079520638566464e-05)\n",
      "(492, 4.7344328777398914e-05)\n",
      "(493, 4.66331621282734e-05)\n",
      "(494, 4.616368096321821e-05)\n",
      "(495, 4.554026236291975e-05)\n",
      "(496, 4.506321783992462e-05)\n",
      "(497, 4.445398735697381e-05)\n",
      "(498, 4.391523543745279e-05)\n",
      "(499, 4.352635733084753e-05)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
