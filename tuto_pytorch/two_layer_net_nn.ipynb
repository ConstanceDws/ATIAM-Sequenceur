{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation uses the nn package from PyTorch to build the network.\n",
    "PyTorch autograd makes it easy to define computational graphs and take gradients,\n",
    "but raw autograd can be a bit too low-level for defining complex neural networks;\n",
    "this is where the nn package can help. The nn package defines a set of Modules,\n",
    "which you can think of as a neural network layer that has produces output from\n",
    "input and may have some trainable weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 635.0342407226562)\n",
      "(1, 587.8619384765625)\n",
      "(2, 546.98193359375)\n",
      "(3, 510.8512268066406)\n",
      "(4, 478.6904296875)\n",
      "(5, 449.91400146484375)\n",
      "(6, 423.64434814453125)\n",
      "(7, 399.5023498535156)\n",
      "(8, 376.93572998046875)\n",
      "(9, 355.8304138183594)\n",
      "(10, 336.098876953125)\n",
      "(11, 317.65545654296875)\n",
      "(12, 300.313232421875)\n",
      "(13, 283.8275146484375)\n",
      "(14, 268.20343017578125)\n",
      "(15, 253.36587524414062)\n",
      "(16, 239.2730712890625)\n",
      "(17, 225.8059844970703)\n",
      "(18, 213.03726196289062)\n",
      "(19, 200.8763885498047)\n",
      "(20, 189.28933715820312)\n",
      "(21, 178.27676391601562)\n",
      "(22, 167.79913330078125)\n",
      "(23, 157.87924194335938)\n",
      "(24, 148.4950714111328)\n",
      "(25, 139.60096740722656)\n",
      "(26, 131.17295837402344)\n",
      "(27, 123.18963623046875)\n",
      "(28, 115.6552963256836)\n",
      "(29, 108.53132629394531)\n",
      "(30, 101.79678344726562)\n",
      "(31, 95.43643951416016)\n",
      "(32, 89.4708251953125)\n",
      "(33, 83.87377166748047)\n",
      "(34, 78.61906433105469)\n",
      "(35, 73.70279693603516)\n",
      "(36, 69.07489013671875)\n",
      "(37, 64.7468032836914)\n",
      "(38, 60.695735931396484)\n",
      "(39, 56.898006439208984)\n",
      "(40, 53.34735870361328)\n",
      "(41, 50.03204345703125)\n",
      "(42, 46.935089111328125)\n",
      "(43, 44.03782272338867)\n",
      "(44, 41.33348846435547)\n",
      "(45, 38.80549240112305)\n",
      "(46, 36.45060348510742)\n",
      "(47, 34.25284957885742)\n",
      "(48, 32.19712448120117)\n",
      "(49, 30.275808334350586)\n",
      "(50, 28.4856014251709)\n",
      "(51, 26.814619064331055)\n",
      "(52, 25.2574405670166)\n",
      "(53, 23.80281639099121)\n",
      "(54, 22.442167282104492)\n",
      "(55, 21.168529510498047)\n",
      "(56, 19.976539611816406)\n",
      "(57, 18.859548568725586)\n",
      "(58, 17.814191818237305)\n",
      "(59, 16.833724975585938)\n",
      "(60, 15.914995193481445)\n",
      "(61, 15.052471160888672)\n",
      "(62, 14.24319839477539)\n",
      "(63, 13.482699394226074)\n",
      "(64, 12.767086029052734)\n",
      "(65, 12.095234870910645)\n",
      "(66, 11.464303016662598)\n",
      "(67, 10.870368003845215)\n",
      "(68, 10.310938835144043)\n",
      "(69, 9.783827781677246)\n",
      "(70, 9.287603378295898)\n",
      "(71, 8.820869445800781)\n",
      "(72, 8.379828453063965)\n",
      "(73, 7.963347911834717)\n",
      "(74, 7.57051420211792)\n",
      "(75, 7.1997528076171875)\n",
      "(76, 6.848998069763184)\n",
      "(77, 6.517333984375)\n",
      "(78, 6.203463077545166)\n",
      "(79, 5.906713485717773)\n",
      "(80, 5.626158714294434)\n",
      "(81, 5.360556125640869)\n",
      "(82, 5.108702182769775)\n",
      "(83, 4.869839668273926)\n",
      "(84, 4.6432623863220215)\n",
      "(85, 4.4283037185668945)\n",
      "(86, 4.2242302894592285)\n",
      "(87, 4.0304341316223145)\n",
      "(88, 3.846390962600708)\n",
      "(89, 3.6716110706329346)\n",
      "(90, 3.5057170391082764)\n",
      "(91, 3.3479156494140625)\n",
      "(92, 3.1977062225341797)\n",
      "(93, 3.0548951625823975)\n",
      "(94, 2.9189646244049072)\n",
      "(95, 2.789647102355957)\n",
      "(96, 2.666525363922119)\n",
      "(97, 2.5492286682128906)\n",
      "(98, 2.4375667572021484)\n",
      "(99, 2.331098794937134)\n",
      "(100, 2.229583501815796)\n",
      "(101, 2.13310170173645)\n",
      "(102, 2.0413994789123535)\n",
      "(103, 1.9540327787399292)\n",
      "(104, 1.8708115816116333)\n",
      "(105, 1.7914116382598877)\n",
      "(106, 1.7155832052230835)\n",
      "(107, 1.6431876420974731)\n",
      "(108, 1.5741641521453857)\n",
      "(109, 1.5082409381866455)\n",
      "(110, 1.4452061653137207)\n",
      "(111, 1.3850274085998535)\n",
      "(112, 1.3276057243347168)\n",
      "(113, 1.2726571559906006)\n",
      "(114, 1.2200745344161987)\n",
      "(115, 1.1698132753372192)\n",
      "(116, 1.1217409372329712)\n",
      "(117, 1.0757392644882202)\n",
      "(118, 1.031752109527588)\n",
      "(119, 0.9896523356437683)\n",
      "(120, 0.9494017958641052)\n",
      "(121, 0.9108887314796448)\n",
      "(122, 0.8740311861038208)\n",
      "(123, 0.8387474417686462)\n",
      "(124, 0.8049780130386353)\n",
      "(125, 0.7726258635520935)\n",
      "(126, 0.7416558861732483)\n",
      "(127, 0.7120346426963806)\n",
      "(128, 0.6836469173431396)\n",
      "(129, 0.656491219997406)\n",
      "(130, 0.6304687857627869)\n",
      "(131, 0.6055276393890381)\n",
      "(132, 0.5816095471382141)\n",
      "(133, 0.5586925745010376)\n",
      "(134, 0.5367411375045776)\n",
      "(135, 0.51567143201828)\n",
      "(136, 0.49548694491386414)\n",
      "(137, 0.4761495590209961)\n",
      "(138, 0.457631915807724)\n",
      "(139, 0.43985679745674133)\n",
      "(140, 0.4228067994117737)\n",
      "(141, 0.4064349830150604)\n",
      "(142, 0.39073652029037476)\n",
      "(143, 0.3756798803806305)\n",
      "(144, 0.3612111508846283)\n",
      "(145, 0.34731805324554443)\n",
      "(146, 0.33398720622062683)\n",
      "(147, 0.32117998600006104)\n",
      "(148, 0.3088759481906891)\n",
      "(149, 0.2970755100250244)\n",
      "(150, 0.28575605154037476)\n",
      "(151, 0.2748688757419586)\n",
      "(152, 0.2644067108631134)\n",
      "(153, 0.2543618381023407)\n",
      "(154, 0.24471622705459595)\n",
      "(155, 0.23544606566429138)\n",
      "(156, 0.22654177248477936)\n",
      "(157, 0.2179834097623825)\n",
      "(158, 0.2097596824169159)\n",
      "(159, 0.20186513662338257)\n",
      "(160, 0.19427390396595)\n",
      "(161, 0.18697357177734375)\n",
      "(162, 0.17996175587177277)\n",
      "(163, 0.1732230931520462)\n",
      "(164, 0.16675062477588654)\n",
      "(165, 0.16052348911762238)\n",
      "(166, 0.15453477203845978)\n",
      "(167, 0.1487763226032257)\n",
      "(168, 0.14323829114437103)\n",
      "(169, 0.13789649307727814)\n",
      "(170, 0.13276250660419464)\n",
      "(171, 0.1278272271156311)\n",
      "(172, 0.12307920306921005)\n",
      "(173, 0.11850820481777191)\n",
      "(174, 0.11411956697702408)\n",
      "(175, 0.10989316552877426)\n",
      "(176, 0.10583090037107468)\n",
      "(177, 0.10192404687404633)\n",
      "(178, 0.09816324710845947)\n",
      "(179, 0.09454809129238129)\n",
      "(180, 0.09106653928756714)\n",
      "(181, 0.08772069215774536)\n",
      "(182, 0.0845038890838623)\n",
      "(183, 0.08140875399112701)\n",
      "(184, 0.07843441516160965)\n",
      "(185, 0.07557429373264313)\n",
      "(186, 0.07282057404518127)\n",
      "(187, 0.07017197459936142)\n",
      "(188, 0.0676209032535553)\n",
      "(189, 0.06516530364751816)\n",
      "(190, 0.06280023604631424)\n",
      "(191, 0.060524795204401016)\n",
      "(192, 0.058334968984127045)\n",
      "(193, 0.05622510984539986)\n",
      "(194, 0.0541934072971344)\n",
      "(195, 0.05223974958062172)\n",
      "(196, 0.050358619540929794)\n",
      "(197, 0.04854661226272583)\n",
      "(198, 0.046800389885902405)\n",
      "(199, 0.04511827230453491)\n",
      "(200, 0.0434998944401741)\n",
      "(201, 0.04194113239645958)\n",
      "(202, 0.040438927710056305)\n",
      "(203, 0.03899211809039116)\n",
      "(204, 0.03759847208857536)\n",
      "(205, 0.036254897713661194)\n",
      "(206, 0.03496179357171059)\n",
      "(207, 0.03371647372841835)\n",
      "(208, 0.032516006380319595)\n",
      "(209, 0.031358323991298676)\n",
      "(210, 0.030243197456002235)\n",
      "(211, 0.02916944958269596)\n",
      "(212, 0.02813616581261158)\n",
      "(213, 0.027139166370034218)\n",
      "(214, 0.02617819234728813)\n",
      "(215, 0.025251997634768486)\n",
      "(216, 0.024360084906220436)\n",
      "(217, 0.023500192910432816)\n",
      "(218, 0.02267126366496086)\n",
      "(219, 0.02187250554561615)\n",
      "(220, 0.02110234461724758)\n",
      "(221, 0.020360242575407028)\n",
      "(222, 0.019643906503915787)\n",
      "(223, 0.018953949213027954)\n",
      "(224, 0.01828804984688759)\n",
      "(225, 0.017646491527557373)\n",
      "(226, 0.01702852174639702)\n",
      "(227, 0.016432102769613266)\n",
      "(228, 0.01585691049695015)\n",
      "(229, 0.015302508138120174)\n",
      "(230, 0.014768446795642376)\n",
      "(231, 0.014253472909331322)\n",
      "(232, 0.013756160624325275)\n",
      "(233, 0.013277098536491394)\n",
      "(234, 0.012814868241548538)\n",
      "(235, 0.012368754483759403)\n",
      "(236, 0.011939088813960552)\n",
      "(237, 0.011524232104420662)\n",
      "(238, 0.011124235577881336)\n",
      "(239, 0.010738479904830456)\n",
      "(240, 0.010366861708462238)\n",
      "(241, 0.010007999837398529)\n",
      "(242, 0.009661702439188957)\n",
      "(243, 0.009327700361609459)\n",
      "(244, 0.00900528859347105)\n",
      "(245, 0.008694460615515709)\n",
      "(246, 0.00839477963745594)\n",
      "(247, 0.008105486631393433)\n",
      "(248, 0.007826209999620914)\n",
      "(249, 0.007556826341897249)\n",
      "(250, 0.007297203876078129)\n",
      "(251, 0.007046529091894627)\n",
      "(252, 0.0068044778890907764)\n",
      "(253, 0.006571087520569563)\n",
      "(254, 0.006345695350319147)\n",
      "(255, 0.006128327455371618)\n",
      "(256, 0.005918454844504595)\n",
      "(257, 0.005715822335332632)\n",
      "(258, 0.00552025344222784)\n",
      "(259, 0.005331534892320633)\n",
      "(260, 0.005149709060788155)\n",
      "(261, 0.0049738334491848946)\n",
      "(262, 0.004804064519703388)\n",
      "(263, 0.004640199244022369)\n",
      "(264, 0.004482155200093985)\n",
      "(265, 0.004329606890678406)\n",
      "(266, 0.004182371310889721)\n",
      "(267, 0.004040233325213194)\n",
      "(268, 0.0039029247127473354)\n",
      "(269, 0.0037703884299844503)\n",
      "(270, 0.003642638213932514)\n",
      "(271, 0.0035191939678043127)\n",
      "(272, 0.003399886190891266)\n",
      "(273, 0.0032848319970071316)\n",
      "(274, 0.0031737382523715496)\n",
      "(275, 0.0030664517544209957)\n",
      "(276, 0.002962819067761302)\n",
      "(277, 0.002862918423488736)\n",
      "(278, 0.002766287187114358)\n",
      "(279, 0.0026729763485491276)\n",
      "(280, 0.002583020832389593)\n",
      "(281, 0.002496129833161831)\n",
      "(282, 0.0024120756424963474)\n",
      "(283, 0.0023309800308197737)\n",
      "(284, 0.002252625534310937)\n",
      "(285, 0.0021769513841718435)\n",
      "(286, 0.0021039261482656)\n",
      "(287, 0.002033383119851351)\n",
      "(288, 0.001965220319107175)\n",
      "(289, 0.00189936812967062)\n",
      "(290, 0.0018357359804213047)\n",
      "(291, 0.0017743100179359317)\n",
      "(292, 0.0017150328494608402)\n",
      "(293, 0.0016577496426180005)\n",
      "(294, 0.0016023804200813174)\n",
      "(295, 0.0015489115612581372)\n",
      "(296, 0.0014972687931731343)\n",
      "(297, 0.001447327435016632)\n",
      "(298, 0.001399087137542665)\n",
      "(299, 0.0013524986570701003)\n",
      "(300, 0.0013075032038614154)\n",
      "(301, 0.0012640507193282247)\n",
      "(302, 0.0012220267672091722)\n",
      "(303, 0.001181495375931263)\n",
      "(304, 0.0011422955431044102)\n",
      "(305, 0.0011044008424505591)\n",
      "(306, 0.0010677933460101485)\n",
      "(307, 0.0010324097238481045)\n",
      "(308, 0.000998229719698429)\n",
      "(309, 0.0009652088629081845)\n",
      "(310, 0.0009332761983387172)\n",
      "(311, 0.0009024286991916597)\n",
      "(312, 0.0008726170635782182)\n",
      "(313, 0.0008437965298071504)\n",
      "(314, 0.0008159825229085982)\n",
      "(315, 0.0007890861015766859)\n",
      "(316, 0.0007630793261341751)\n",
      "(317, 0.0007379369344562292)\n",
      "(318, 0.0007136365748010576)\n",
      "(319, 0.0006901540327817202)\n",
      "(320, 0.0006674575270153582)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(321, 0.0006455223192460835)\n",
      "(322, 0.0006243234383873641)\n",
      "(323, 0.0006038250285200775)\n",
      "(324, 0.0005840200465172529)\n",
      "(325, 0.0005648540682159364)\n",
      "(326, 0.0005463585257530212)\n",
      "(327, 0.0005284683429636061)\n",
      "(328, 0.0005111755453981459)\n",
      "(329, 0.0004944488173350692)\n",
      "(330, 0.00047828268725425005)\n",
      "(331, 0.00046264295815490186)\n",
      "(332, 0.000447513913968578)\n",
      "(333, 0.00043291668407619)\n",
      "(334, 0.0004187883751001209)\n",
      "(335, 0.00040513233398087323)\n",
      "(336, 0.00039193968405015767)\n",
      "(337, 0.00037916842848062515)\n",
      "(338, 0.00036683661164715886)\n",
      "(339, 0.0003549003158695996)\n",
      "(340, 0.00034335791133344173)\n",
      "(341, 0.00033218442695215344)\n",
      "(342, 0.00032138440292328596)\n",
      "(343, 0.0003109546087216586)\n",
      "(344, 0.00030086320475675166)\n",
      "(345, 0.0002911035262513906)\n",
      "(346, 0.000281664018984884)\n",
      "(347, 0.00027252594009041786)\n",
      "(348, 0.0002637002326082438)\n",
      "(349, 0.00025515691959299147)\n",
      "(350, 0.00024690499412827194)\n",
      "(351, 0.00023892168246675283)\n",
      "(352, 0.00023119623074308038)\n",
      "(353, 0.00022372548119165003)\n",
      "(354, 0.0002165039477404207)\n",
      "(355, 0.00020951824262738228)\n",
      "(356, 0.00020274650887586176)\n",
      "(357, 0.0001962092355825007)\n",
      "(358, 0.00018988733063451946)\n",
      "(359, 0.0001837650779634714)\n",
      "(360, 0.0001778474688762799)\n",
      "(361, 0.0001721156877465546)\n",
      "(362, 0.00016657808737363666)\n",
      "(363, 0.00016122583474498242)\n",
      "(364, 0.00015604420332238078)\n",
      "(365, 0.0001510247529949993)\n",
      "(366, 0.00014617435226682574)\n",
      "(367, 0.00014148323680274189)\n",
      "(368, 0.0001369456440443173)\n",
      "(369, 0.0001325493649346754)\n",
      "(370, 0.0001283010933548212)\n",
      "(371, 0.0001241869031218812)\n",
      "(372, 0.00012021063594147563)\n",
      "(373, 0.00011637076386250556)\n",
      "(374, 0.00011265541252214462)\n",
      "(375, 0.000109060165414121)\n",
      "(376, 0.0001055918910424225)\n",
      "(377, 0.00010222391574643552)\n",
      "(378, 9.896121628116816e-05)\n",
      "(379, 9.581325139151886e-05)\n",
      "(380, 9.276369382860139e-05)\n",
      "(381, 8.981023711385205e-05)\n",
      "(382, 8.695334690855816e-05)\n",
      "(383, 8.41906585264951e-05)\n",
      "(384, 8.151385554810986e-05)\n",
      "(385, 7.893084693932906e-05)\n",
      "(386, 7.642399577889591e-05)\n",
      "(387, 7.399553578579798e-05)\n",
      "(388, 7.165086572058499e-05)\n",
      "(389, 6.937969737919047e-05)\n",
      "(390, 6.718008080497384e-05)\n",
      "(391, 6.505323835881427e-05)\n",
      "(392, 6.299238884821534e-05)\n",
      "(393, 6.099704478401691e-05)\n",
      "(394, 5.906627484364435e-05)\n",
      "(395, 5.720021727029234e-05)\n",
      "(396, 5.539472113014199e-05)\n",
      "(397, 5.364305980037898e-05)\n",
      "(398, 5.19499481015373e-05)\n",
      "(399, 5.030741886002943e-05)\n",
      "(400, 4.871967757935636e-05)\n",
      "(401, 4.7182573325699195e-05)\n",
      "(402, 4.56962879979983e-05)\n",
      "(403, 4.425513543537818e-05)\n",
      "(404, 4.2858486267505214e-05)\n",
      "(405, 4.151101165916771e-05)\n",
      "(406, 4.020567939733155e-05)\n",
      "(407, 3.8940601370995864e-05)\n",
      "(408, 3.771042247535661e-05)\n",
      "(409, 3.652824671007693e-05)\n",
      "(410, 3.5379587643546984e-05)\n",
      "(411, 3.42656385328155e-05)\n",
      "(412, 3.318911694805138e-05)\n",
      "(413, 3.2145533623406664e-05)\n",
      "(414, 3.11406547552906e-05)\n",
      "(415, 3.0162447728798725e-05)\n",
      "(416, 2.921915438491851e-05)\n",
      "(417, 2.8304199076956138e-05)\n",
      "(418, 2.7415104341343977e-05)\n",
      "(419, 2.6554213036433794e-05)\n",
      "(420, 2.5723800717969425e-05)\n",
      "(421, 2.491832674422767e-05)\n",
      "(422, 2.4138260414474644e-05)\n",
      "(423, 2.338403464818839e-05)\n",
      "(424, 2.2652786356047727e-05)\n",
      "(425, 2.194559965573717e-05)\n",
      "(426, 2.1258349079289474e-05)\n",
      "(427, 2.0594627130776644e-05)\n",
      "(428, 1.994997546717059e-05)\n",
      "(429, 1.9328690541442484e-05)\n",
      "(430, 1.872797292890027e-05)\n",
      "(431, 1.8141518012271263e-05)\n",
      "(432, 1.7576609025127254e-05)\n",
      "(433, 1.7028458387358114e-05)\n",
      "(434, 1.6497968317708e-05)\n",
      "(435, 1.598548624315299e-05)\n",
      "(436, 1.548789623484481e-05)\n",
      "(437, 1.500403323007049e-05)\n",
      "(438, 1.4538016330334358e-05)\n",
      "(439, 1.408579100825591e-05)\n",
      "(440, 1.3646033949044067e-05)\n",
      "(441, 1.322132538916776e-05)\n",
      "(442, 1.2812291060981806e-05)\n",
      "(443, 1.2414279808581341e-05)\n",
      "(444, 1.2028055607515853e-05)\n",
      "(445, 1.1655478374450468e-05)\n",
      "(446, 1.129309839598136e-05)\n",
      "(447, 1.0943010238406714e-05)\n",
      "(448, 1.0602901966194622e-05)\n",
      "(449, 1.0275423846906051e-05)\n",
      "(450, 9.956966096069664e-06)\n",
      "(451, 9.649297680880409e-06)\n",
      "(452, 9.349529136670753e-06)\n",
      "(453, 9.059654075826984e-06)\n",
      "(454, 8.780622010817751e-06)\n",
      "(455, 8.508477549185045e-06)\n",
      "(456, 8.244775926868897e-06)\n",
      "(457, 7.990740414243191e-06)\n",
      "(458, 7.743798050796613e-06)\n",
      "(459, 7.5044144978164695e-06)\n",
      "(460, 7.272129550983664e-06)\n",
      "(461, 7.04789954397711e-06)\n",
      "(462, 6.83072039464605e-06)\n",
      "(463, 6.619414762099041e-06)\n",
      "(464, 6.415115421987139e-06)\n",
      "(465, 6.2169492593966424e-06)\n",
      "(466, 6.025331458658911e-06)\n",
      "(467, 5.839006462338148e-06)\n",
      "(468, 5.658991540258285e-06)\n",
      "(469, 5.485710516950348e-06)\n",
      "(470, 5.317203886079369e-06)\n",
      "(471, 5.152846370037878e-06)\n",
      "(472, 4.994664777768776e-06)\n",
      "(473, 4.840682322537759e-06)\n",
      "(474, 4.69230008093291e-06)\n",
      "(475, 4.5472361307474785e-06)\n",
      "(476, 4.408130280353362e-06)\n",
      "(477, 4.272239948477363e-06)\n",
      "(478, 4.140536930208327e-06)\n",
      "(479, 4.0138315853255335e-06)\n",
      "(480, 3.8914931792533025e-06)\n",
      "(481, 3.772292529902188e-06)\n",
      "(482, 3.6558203646563925e-06)\n",
      "(483, 3.5438781651464524e-06)\n",
      "(484, 3.435005055507645e-06)\n",
      "(485, 3.329791752548772e-06)\n",
      "(486, 3.2267666938423645e-06)\n",
      "(487, 3.1288975606003078e-06)\n",
      "(488, 3.0329440505738603e-06)\n",
      "(489, 2.9399914183159126e-06)\n",
      "(490, 2.8495112474047346e-06)\n",
      "(491, 2.7626551855064463e-06)\n",
      "(492, 2.6786199214257067e-06)\n",
      "(493, 2.5964227461372502e-06)\n",
      "(494, 2.5168396859953646e-06)\n",
      "(495, 2.440264324832242e-06)\n",
      "(496, 2.3657917154196184e-06)\n",
      "(497, 2.293156740051927e-06)\n",
      "(498, 2.2233846266317414e-06)\n",
      "(499, 2.1556131741817808e-06)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
